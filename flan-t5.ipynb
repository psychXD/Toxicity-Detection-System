{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T17:40:53.382337Z","iopub.execute_input":"2025-01-08T17:40:53.382624Z","iopub.status.idle":"2025-01-08T17:40:53.699321Z","shell.execute_reply.started":"2025-01-08T17:40:53.382602Z","shell.execute_reply":"2025-01-08T17:40:53.698392Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/hate-speech-and-offensive-language-dataset/labeled_data.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install peft==0.14.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T17:40:53.937397Z","iopub.execute_input":"2025-01-08T17:40:53.937815Z","iopub.status.idle":"2025-01-08T17:40:59.814856Z","shell.execute_reply.started":"2025-01-08T17:40:53.937783Z","shell.execute_reply":"2025-01-08T17:40:59.813592Z"}},"outputs":[{"name":"stdout","text":"Collecting peft==0.14.0\n  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.14.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.14.0) (24.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.14.0) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.14.0) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.14.0) (2.4.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.14.0) (4.44.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.14.0) (4.66.5)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.14.0) (0.34.2)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.14.0) (0.4.5)\nCollecting huggingface-hub>=0.25.0 (from peft==0.14.0)\n  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft==0.14.0) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft==0.14.0) (2024.6.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft==0.14.0) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft==0.14.0) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.14.0) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.14.0) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.14.0) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.14.0) (2024.9.11)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.14.0) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.14.0) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft==0.14.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft==0.14.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft==0.14.0) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft==0.14.0) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.14.0) (1.3.0)\nDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.7/450.7 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface-hub, peft\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.24.7\n    Uninstalling huggingface-hub-0.24.7:\n      Successfully uninstalled huggingface-hub-0.24.7\nSuccessfully installed huggingface-hub-0.27.1 peft-0.14.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"pip install --upgrade transformers datasets huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T17:40:59.816492Z","iopub.execute_input":"2025-01-08T17:40:59.816825Z","iopub.status.idle":"2025-01-08T17:41:14.369507Z","shell.execute_reply.started":"2025-01-08T17:40:59.816791Z","shell.execute_reply":"2025-01-08T17:41:14.368506Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nCollecting transformers\n  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.27.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.2\n    Uninstalling transformers-4.44.2:\n      Successfully uninstalled transformers-4.44.2\nSuccessfully installed tokenizers-0.21.0 transformers-4.47.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import re\nimport torch\nimport pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\nfrom peft import LoraConfig, get_peft_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T17:41:14.371277Z","iopub.execute_input":"2025-01-08T17:41:14.371502Z","iopub.status.idle":"2025-01-08T17:41:27.466802Z","shell.execute_reply.started":"2025-01-08T17:41:14.371484Z","shell.execute_reply":"2025-01-08T17:41:27.466147Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"dataPath = '../input/hate-speech-and-offensive-language-dataset/labeled_data.csv'\n\ndf = pd.read_csv(dataPath)\n\ndef clean_text(text):\n    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n    text = re.sub(r\"&[a-z]+;\", \"\", text)  # Remove HTML entities\n    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters\n    return text.lower()\n\nclass_mapping = {\n    0: \"hate speech\",\n    1: \"offensive\",\n    2: \"neutral\"\n}\n\ndf[\"cleanedTweet\"] = df[\"tweet\"].apply(clean_text)\ndf[\"input_text\"] = \"classify: \" + df[\"cleanedTweet\"]\ndf[\"output_text\"] = df[\"class\"].map(class_mapping)\n\ndf = df.dropna(subset=[\"output_text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T17:41:27.468120Z","iopub.execute_input":"2025-01-08T17:41:27.468660Z","iopub.status.idle":"2025-01-08T17:41:27.728343Z","shell.execute_reply.started":"2025-01-08T17:41:27.468635Z","shell.execute_reply":"2025-01-08T17:41:27.727532Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T17:41:27.729365Z","iopub.execute_input":"2025-01-08T17:41:27.729738Z","iopub.status.idle":"2025-01-08T17:41:27.740321Z","shell.execute_reply.started":"2025-01-08T17:41:27.729700Z","shell.execute_reply":"2025-01-08T17:41:27.739261Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T17:41:27.741420Z","iopub.execute_input":"2025-01-08T17:41:27.741723Z","iopub.status.idle":"2025-01-08T17:41:30.951431Z","shell.execute_reply.started":"2025-01-08T17:41:27.741699Z","shell.execute_reply":"2025-01-08T17:41:30.950564Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb62d641e13c4fa99566af7e0ccb557f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8620efc3904b4aca8600e60108c95f14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"902fa5cd3dd84a71856d84263c8019de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b9028305fb04cab8afbb74aa35fe7fc"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32d15168ad7d4ca5a7605885f81e6b9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"244e8541b9a34d01b0b1d0006f3581e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec34b760f931465da2115152a38197f3"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"lora_config = LoraConfig(\n    task_type=\"SEQ_2_SEQ_LM\",\n    r=8,\n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],  \n    lora_dropout=0.1,\n    bias=\"none\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T17:41:30.952500Z","iopub.execute_input":"2025-01-08T17:41:30.952733Z","iopub.status.idle":"2025-01-08T17:41:30.957115Z","shell.execute_reply.started":"2025-01-08T17:41:30.952712Z","shell.execute_reply":"2025-01-08T17:41:30.956016Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"peft_model = get_peft_model(model, lora_config)\n\n#Tokenize data\ndef tokenize_data(row):\n    inputs = tokenizer(\n        row[\"input_text\"],\n        max_length=128,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n    labels = tokenizer(\n        row[\"output_text\"],\n        max_length=32,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )[\"input_ids\"]\n    labels[labels == tokenizer.pad_token_id] = -100  # Mask padding tokens for loss\n    return {\n        \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n        \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n        \"labels\": labels.squeeze(0),\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T17:41:30.959644Z","iopub.execute_input":"2025-01-08T17:41:30.959847Z","iopub.status.idle":"2025-01-08T17:41:31.042410Z","shell.execute_reply.started":"2025-01-08T17:41:30.959830Z","shell.execute_reply":"2025-01-08T17:41:31.041791Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Apply tokenization \ntrain_tokenized = train_df.apply(tokenize_data, axis=1)\ntest_tokenized = test_df.apply(tokenize_data, axis=1)\n\n# ist of dictionaries\ntrain_tokenized = train_tokenized.tolist()\ntest_tokenized = test_tokenized.tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T17:41:31.043465Z","iopub.execute_input":"2025-01-08T17:41:31.043758Z","iopub.status.idle":"2025-01-08T17:41:46.075492Z","shell.execute_reply.started":"2025-01-08T17:41:31.043727Z","shell.execute_reply":"2025-01-08T17:41:46.074466Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"#custom dataset\nclass CustomDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T17:41:46.076576Z","iopub.execute_input":"2025-01-08T17:41:46.076913Z","iopub.status.idle":"2025-01-08T17:41:46.082371Z","shell.execute_reply.started":"2025-01-08T17:41:46.076866Z","shell.execute_reply":"2025-01-08T17:41:46.081524Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"#datasets and dataloaders\ntrain_dataset = CustomDataset(train_tokenized)\ntest_dataset = CustomDataset(test_tokenized)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=8)\n\n#Device setup for single GPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\npeft_model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T17:41:46.083277Z","iopub.execute_input":"2025-01-08T17:41:46.083587Z","iopub.status.idle":"2025-01-08T17:41:46.609987Z","shell.execute_reply.started":"2025-01-08T17:41:46.083549Z","shell.execute_reply":"2025-01-08T17:41:46.609114Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"PeftModelForSeq2SeqLM(\n  (base_model): LoraModel(\n    (model): T5ForConditionalGeneration(\n      (shared): Embedding(32128, 512)\n      (encoder): T5Stack(\n        (embed_tokens): Embedding(32128, 512)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): lora.Linear(\n                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=512, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=384, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k): Linear(in_features=512, out_features=384, bias=False)\n                  (v): lora.Linear(\n                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=512, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=384, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o): Linear(in_features=384, out_features=512, bias=False)\n                  (relative_attention_bias): Embedding(32, 6)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1-7): 7 x T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): lora.Linear(\n                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=512, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=384, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k): Linear(in_features=512, out_features=384, bias=False)\n                  (v): lora.Linear(\n                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=512, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=384, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o): Linear(in_features=384, out_features=512, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (decoder): T5Stack(\n        (embed_tokens): Embedding(32128, 512)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): lora.Linear(\n                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=512, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=384, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k): Linear(in_features=512, out_features=384, bias=False)\n                  (v): lora.Linear(\n                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=512, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=384, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o): Linear(in_features=384, out_features=512, bias=False)\n                  (relative_attention_bias): Embedding(32, 6)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): lora.Linear(\n                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=512, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=384, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k): Linear(in_features=512, out_features=384, bias=False)\n                  (v): lora.Linear(\n                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=512, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=384, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o): Linear(in_features=384, out_features=512, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1-7): 7 x T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): lora.Linear(\n                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=512, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=384, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k): Linear(in_features=512, out_features=384, bias=False)\n                  (v): lora.Linear(\n                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=512, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=384, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o): Linear(in_features=384, out_features=512, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): lora.Linear(\n                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=512, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=384, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k): Linear(in_features=512, out_features=384, bias=False)\n                  (v): lora.Linear(\n                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=512, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=384, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o): Linear(in_features=384, out_features=512, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"optimizer = AdamW(filter(lambda p: p.requires_grad, peft_model.parameters()), lr=5e-5)\n\n#Training Loop\nepochs = 6\npeft_model.train()\n\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}/{epochs}\")\n    total_loss = 0\n    for batch in train_dataloader:\n        optimizer.zero_grad()\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = peft_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        loss = outputs.loss.mean()  # Ensure scalar loss\n        total_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n\n    average_loss = total_loss / len(train_dataloader)\n    print(f\"Average Loss after epoch {epoch + 1}: {average_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T17:41:46.610841Z","iopub.execute_input":"2025-01-08T17:41:46.611112Z","iopub.status.idle":"2025-01-08T18:03:52.812256Z","shell.execute_reply.started":"2025-01-08T17:41:46.611079Z","shell.execute_reply":"2025-01-08T18:03:52.811286Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/6\n","output_type":"stream"},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"name":"stdout","text":"Average Loss after epoch 1: 0.7850\nEpoch 2/6\nAverage Loss after epoch 2: 0.2449\nEpoch 3/6\nAverage Loss after epoch 3: 0.1964\nEpoch 4/6\nAverage Loss after epoch 4: 0.1754\nEpoch 5/6\nAverage Loss after epoch 5: 0.1635\nEpoch 6/6\nAverage Loss after epoch 6: 0.1570\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"peft_model.eval()\nall_predictions = []\nall_true_labels = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # Generate predictions\n        generated_ids = peft_model.generate(input_ids=input_ids, attention_mask=attention_mask)\n        preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n\n        # Filter out -100 from labels before decoding\n        labels[labels == -100] = tokenizer.pad_token_id\n        true_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n        all_predictions.extend(preds)\n        all_true_labels.extend(true_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T18:05:37.843056Z","iopub.execute_input":"2025-01-08T18:05:37.843358Z","iopub.status.idle":"2025-01-08T18:06:28.988495Z","shell.execute_reply.started":"2025-01-08T18:05:37.843338Z","shell.execute_reply":"2025-01-08T18:06:28.987575Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"print(\"Classification Report:\")\nprint(classification_report(all_true_labels, all_predictions))\n\noutput_dir = \"/kaggle/working/flan_t5_peft_model\"\npeft_model.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(f\"Model and tokenizer saved to {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T18:06:28.989985Z","iopub.execute_input":"2025-01-08T18:06:28.990377Z","iopub.status.idle":"2025-01-08T18:06:29.244558Z","shell.execute_reply.started":"2025-01-08T18:06:28.990341Z","shell.execute_reply":"2025-01-08T18:06:29.243732Z"}},"outputs":[{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n hate speech       0.47      0.23      0.31       290\n     neutral       0.81      0.92      0.86       835\n   offensive       0.94      0.95      0.94      3832\n\n    accuracy                           0.90      4957\n   macro avg       0.74      0.70      0.70      4957\nweighted avg       0.89      0.90      0.89      4957\n\nModel and tokenizer saved to /kaggle/working/flan_t5_peft_model\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}